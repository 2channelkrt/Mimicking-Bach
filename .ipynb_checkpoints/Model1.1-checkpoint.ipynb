{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import csvFile as csv\n",
    "import numpy as np\n",
    "\n",
    "#########################user configuration variables############################\n",
    "#defining basic file paths. Directories for default settings are already created, which are named 'midi', 'csv', 'out'\n",
    "#'midi' : where raw training data is stored\n",
    "#'csv' : where converted training data is stored\n",
    "#'out' : where created csv & midi files are stored\n",
    "\n",
    "basePath=os.getcwd()## default work path will be here.\n",
    "midiFileRelativePath=r'\\midicsv-1.1\\midi'#where training data is stored\n",
    "midicsvRelPath=r'\\midicsv-1.1'#midicsv location\n",
    "csvRelPath=r'\\midicsv-1.1\\csv'#where csv file will be stored\n",
    "#################################################################################\n",
    "#defining absolute path...\n",
    "midiFileAbsPath=basePath+midiFileRelativePath\n",
    "midicsvWorkingPath=basePath+midicsvRelPath\n",
    "csvAbsPath=basePath+csvRelPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "raw csv files created\n",
      "96 csv files detected\n"
     ]
    }
   ],
   "source": [
    "files=sorted(os.listdir(midiFileAbsPath))#retreving training data\n",
    "\n",
    "#converting training data midi files for convenience.\n",
    "#This naming onvention will be used for rest of the process\n",
    "if(files[0]!='0.mid'):\n",
    "    print('filenames not converted. Converting...')\n",
    "    os.chdir(midiFileAbsPath)\n",
    "    i=0\n",
    "    for fileName in files:\n",
    "        os.rename(fileName,str(i)+'.mid')\n",
    "        i+=1\n",
    "    print(\"filenames converted\")\n",
    "\n",
    "files=sorted(os.listdir(midiFileAbsPath))\n",
    "#creating csv files from training data, using 'midicsv'\n",
    "checkFiles=sorted(os.listdir(csvAbsPath))\n",
    "print(checkFiles)\n",
    "if(len(checkFiles)==0):\n",
    "    os.chdir(midicsvWorkingPath)\n",
    "    for fileName in files:\n",
    "        newFileName=fileName[:-4]\n",
    "        cmd='midicsv ./midi/'+fileName+' ./csv/'+newFileName+'.csv'\n",
    "        os.system(cmd)\n",
    "        print('raw csv files created')\n",
    "else:\n",
    "     print(\"csv files already created\")\n",
    "\n",
    "fileList=sorted(os.listdir(csvAbsPath))\n",
    "print(\"%d csv files detected\"% len(fileList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading all csv data, and retreving metatada.\n",
    "myFile=[]\n",
    "os.chdir(csvAbsPath)\n",
    "for files in fileList:\n",
    "    myFile.append(csv.csvFile(files))\n",
    "    myFile[len(myFile)-1].convert2InputFormat()\n",
    "    myFile[len(myFile)-1].getBasicTimeStep()\n",
    "    myFile[len(myFile)-1].createInputData1()\n",
    "    myFile[len(myFile)-1].createInputData1()\n",
    "    #myFile[len(myFile)-1].convert2OutputFormat(convertedAbsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60, 77], [60, 73], [60, 68], [60, 73], [60, 77], [60, 73], [60, 78], [60, 73], [60, 78], [60, 73], [60, 78], [60, 73], [60, 80], [60, 73], [60, 80], [60, 73], [60, 80], [60, 73], [60, 82], [60, 73], [60, 82], [60, 73], [60, 82], [60, 73], [60, 80], [60, 73], [60, 80], [60, 73], [60, 80], [60, 73], [60, 78], [60, 77], [60, 75], [60, 77], [60, 78], [60, 75], [60, 77], [60, 75], [60, 73], [60, 75], [60, 77], [60, 73], [60, 75], [60, 77], [60, 75], [60, 73], [60, 72], [60, 70], [240, 68], [120, 80], [240, 70], [120, 80], [240, 72], [120, 80], [240, 73], [120, 80], [240, 72], [120, 80], [240, 70], [120, 79], [240, 68], [240, 80], [60, 78], [60, 77], [60, 75], [60, 77], [60, 78], [60, 75], [60, 70], [60, 75], [60, 78], [60, 75], [60, 80], [60, 75], [60, 80], [60, 75], [60, 80], [60, 75], [60, 82], [60, 75], [60, 82], [60, 75], [60, 82], [60, 75], [60, 83], [60, 75], [60, 83], [60, 75], [60, 83], [60, 75], [60, 82], [60, 75], [60, 82], [60, 75], [60, 82], [60, 75], [60, 80], [60, 78], [60, 77], [60, 78], [60, 80], [60, 77], [60, 78], [60, 77], [60, 75], [60, 77], [60, 78], [60, 75], [60, 77], [60, 78], [60, 77], [60, 75], [60, 73], [60, 72], [240, 70], [120, 82], [240, 72], [120, 82], [240, 73], [120, 82], [240, 75], [120, 82], [240, 73], [120, 82], [240, 72], [120, 81], [240, 70], [240, 82], [120, 80], [120, 79], [120, 80], [120, 68], [240, 80], [120, 78], [120, 77], [60, 78], [60, 77], [60, 75], [60, 77], [60, 78], [60, 75], [60, 81], [60, 79], [60, 77], [60, 79], [60, 81], [60, 77], [60, 82], [60, 81], [60, 82], [60, 84], [60, 82], [60, 80], [60, 79], [60, 77], [60, 75], [60, 77], [60, 79], [60, 75], [120, 80], [120, 68], [240, 80], [120, 78], [120, 77], [120, 78], [120, 66], [240, 78], [120, 77], [120, 75], [60, 77], [60, 75], [60, 73], [60, 75], [60, 77], [60, 73], [60, 79], [60, 77], [60, 75], [60, 77], [60, 79], [60, 75], [60, 80], [60, 79], [60, 80], [60, 82], [60, 80], [60, 78], [60, 77], [60, 75], [60, 73], [60, 75], [60, 77], [60, 73], [240, 66], [120, 78], [240, 68], [120, 78], [240, 70], [120, 78], [240, 71], [120, 78], [240, 70], [120, 78], [240, 68], [120, 77], [240, 66], [240, 78], [60, 77], [60, 75], [60, 73], [60, 75], [60, 77], [60, 73], [60, 68], [60, 73], [60, 77], [60, 73], [60, 78], [60, 73], [60, 78], [60, 73], [60, 78], [60, 73], [60, 80], [60, 73], [60, 80], [60, 73], [60, 80], [60, 73], [60, 82], [60, 73], [60, 82], [60, 73], [60, 82], [60, 73], [60, 80], [60, 73], [60, 80], [60, 73], [60, 80], [60, 73], [60, 78], [60, 77], [60, 75], [60, 77], [60, 78], [60, 75], [60, 77], [60, 75], [60, 73], [60, 75], [60, 77], [60, 73], [60, 75], [60, 77], [60, 75], [60, 73], [60, 72], [60, 70], [60, 72], [120, 68], [120, 68], [60, 68], [60, 72], [120, 68], [120, 68], [60, 68], [60, 73], [120, 68], [120, 68], [60, 68], [60, 73], [120, 68], [120, 68], [60, 68], [60, 78], [120, 68], [120, 68], [60, 68], [60, 78], [120, 68], [120, 68], [60, 68], [60, 77], [120, 68], [120, 68], [60, 68], [60, 77], [120, 68], [120, 68], [60, 68], [60, 79], [120, 70], [120, 70], [60, 70], [60, 79], [120, 70], [120, 70], [60, 70], [60, 80], [120, 72], [120, 72], [60, 72], [60, 80], [120, 72], [120, 72], [60, 72], [60, 80], [120, 73], [120, 68], [120, 73], [120, 77], [120, 80], [60, 82], [60, 83], [60, 82], [60, 80], [60, 78], [60, 77], [60, 75], [60, 77], [60, 78], [60, 80], [60, 83], [60, 82], [60, 80], [60, 82], [120, 75], [120, 66], [120, 70], [120, 75], [120, 78], [60, 80], [60, 81], [60, 80], [60, 78], [60, 76], [60, 75], [60, 73], [60, 75], [60, 76], [60, 78], [60, 81], [60, 80], [60, 78], [60, 80], [120, 73], [120, 76], [120, 73], [120, 69], [120, 66], [120, 75], [120, 68], [120, 64], [120, 73], [120, 69], [120, 66], [60, 63], [60, 60], [120, 56], [120, 56], [60, 56], [60, 60], [120, 56], [120, 56], [60, 56], [60, 61], [120, 56], [120, 56], [60, 56], [60, 61], [120, 56], [120, 56], [60, 56], [60, 66], [120, 56], [120, 56], [60, 56], [60, 66], [120, 56], [120, 56], [60, 56], [60, 65], [120, 56], [120, 56], [60, 56], [60, 65], [120, 56], [120, 56], [60, 56], [60, 67], [120, 58], [120, 58], [60, 58], [60, 67], [120, 58], [120, 58], [420, 58], [60, 61], [60, 64], [60, 67], [60, 70], [60, 73], [60, 76], [60, 72], [60, 75], [60, 78], [60, 75], [60, 72], [420, 68], [60, 65], [60, 68], [60, 73], [60, 68], [60, 65], [540, 61], [120, 65], [120, 63]]\n"
     ]
    }
   ],
   "source": [
    "#let's test out if training data is imported correctly.\n",
    "#data should start with something like this...\n",
    "#[[60, 77], [60, 73], [60, 68], [60, 73], [60, 77], [60, 73], ...\n",
    "print(myFile[2].inputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for integrating all seperate training data\n",
    "raw_data=[]\n",
    "for f in myFile:\n",
    "    raw_data.append(f.inputData)\n",
    "\n",
    "def createBeatLib(data):\n",
    "    _dict={}\n",
    "    for track in data:\n",
    "        for beat, _ in track:\n",
    "            if beat not in _dict:\n",
    "                _dict[beat]=1\n",
    "            else:\n",
    "                _dict[beat]+=1\n",
    "    return _dict\n",
    "def createNoteLib(data):\n",
    "    _dict={}\n",
    "    for track in data:\n",
    "        for _, note in track:\n",
    "            if note not in _dict:\n",
    "                _dict[note]=1\n",
    "            else:\n",
    "                _dict[note]+=1\n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{60: 20783, 68: 5, 30: 2607, 22: 454, 23: 462, 67: 8, 120: 8591, 720: 168, 780: 31, 240: 1982, 300: 344, 180: 733, 26: 65, 19: 255, 18: 80, 15: 263, 540: 97, 480: 662, 360: 702, 420: 98, 270: 12, 90: 74, 1080: 32, 600: 185, 1020: 7, 75: 11, 840: 69, 45: 151, 1260: 3, 660: 8, 165: 20, 34: 19, 41: 9, 124: 7, 596: 3, 27: 21, 191: 2, 38: 68, 2760: 2, 1200: 33, 900: 1, 1196: 1, 476: 1, 236: 2, 364: 1, 1800: 4, 2520: 3, 960: 77, 101: 4, 645: 2, 135: 13, 105: 1, 255: 1, 1560: 12, 735: 1, 37: 62, 8: 6, 11: 21, 12: 5, 52: 5, 1920: 10, 2040: 5, 3480: 4, 1440: 9, 150: 50, 118: 3, 302: 1, 390: 6, 24: 5, 25: 8, 58: 2, 71: 2, 109: 2, 20: 8, 21: 1, 40: 1170, 280: 1, 2340: 2, 225: 2, 330: 17, 210: 6, 1050: 1, 570: 3, 199: 1, 510: 2, 2160: 5, 3120: 1, 82: 7, 160: 22, 98: 1, 262: 1, 1500: 3, 33: 1, 49: 4, 619: 1, 2220: 1, 1320: 5, 232: 1, 750: 1, 1980: 1, 3000: 3, 17: 2, 9: 1, 127: 1, 94: 1, 142: 8, 5040: 1, 53: 2, 1680: 8, 12480: 1, 12000: 1, 187: 1, 1140: 1, 760: 1, 80: 377, 2880: 2, 4800: 1, 3840: 3, 3960: 1, 320: 16, 560: 10, 800: 4, 640: 8, 43: 1, 77: 1, 1040: 2, 1280: 1, 400: 4, 5: 1, 7: 1, 8160: 1, 1620: 1, 13560: 1, 4320: 1, 630: 1, 2640: 2, 8640: 1, 7680: 1}\n"
     ]
    }
   ],
   "source": [
    "#let's check raw beat data\n",
    "    #key: length of the beat\n",
    "    #value: occurance frequency\n",
    "print(createBeatLib(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################user configuration variables############################\n",
    "#This model tries to cut out some 'exceptionally rare' notes for following reasons\n",
    "    #1. this note might be from chorus, not melody\n",
    "    #2. number of parameters become too big\n",
    "cuttingEdge=100 #beat with occurance frequency below this threshold will be transposed or omitted\n",
    "\n",
    "#transpose\n",
    "#if note is considered 'exceptionally rare', that note will be shifted to be\n",
    "#shorter or longer beat. Since 1 beat is in terms of milliseconds for this project, small shift tends to be unnoticable.\n",
    "#this helps to tolerate minor rounding error of original csv data converted from midi files.\n",
    "\n",
    "transposeHigh=5 # transpose upper threshold\n",
    "transposeLow=5 # transpose lower threshold\n",
    "#################################################################################\n",
    "#This transposes all data in terms of note height. keys below 36 was never used.\n",
    "noteTransPose=36\n",
    "\n",
    "total_data=[]\n",
    "#actually integrating all seperate training data\n",
    "beatLib=createBeatLib(raw_data)\n",
    "\n",
    "for track in raw_data:\n",
    "    newTrack=[]\n",
    "    for beat, note in track:\n",
    "        if(beatLib[beat]>cuttingEdge):\n",
    "            newTrack.append([beat,note-noteTransPose])\n",
    "        else:\n",
    "            for i in range(-transposeLow,transposeHigh):\n",
    "                if((beat+i) in beatLib and beatLib[(beat+i)]>cuttingEdge):\n",
    "                    newTrack.append([(beat+i),note-noteTransPose])\n",
    "                    break\n",
    "    total_data.append(newTrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################user configuration variables############################\n",
    "#length of the training & creating melody. In terms of 'note events'\n",
    "seqLength=100\n",
    "#################################################################################\n",
    "new_total_data=[]\n",
    "\n",
    "#leftover data is not used\n",
    "for track in total_data:\n",
    "    _size=len(track)//seqLength\n",
    "    for i in range(_size):\n",
    "        new_total_data.append(track[i*seqLength:(i+1)*seqLength+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355\n"
     ]
    }
   ],
   "source": [
    "print(len(new_total_data))\n",
    "\n",
    "data_limiter=350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating shifted index for the output part of the model.\n",
    "\n",
    "total_data_x=[]\n",
    "total_data_y=[]\n",
    "def shift(x):\n",
    "    out=[]\n",
    "    for i in range(len(x)):\n",
    "        if(i==0):\n",
    "            continue\n",
    "        out.append(x[i])\n",
    "    return out\n",
    "\n",
    "for i in new_total_data:\n",
    "    total_data_y.append(shift(i))\n",
    "\n",
    "for i in new_total_data:\n",
    "    total_data_x.append(i[:-1])\n",
    "\n",
    "train_x=total_data_x[:350]\n",
    "train_y=total_data_y[:350]\n",
    "\n",
    "test_x=total_data_x[350:]\n",
    "test_y=total_data_y[350:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41199\n",
      "40297\n",
      "-902\n",
      "17\n",
      "96\n",
      "442\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#comparison between optimized & raw data\n",
    "newBeatLib=createBeatLib(total_data)\n",
    "\n",
    "print(sum(beatLib.values()))\n",
    "print(sum(newBeatLib.values()))\n",
    "print(sum(newBeatLib.values())-sum(beatLib.values()))\n",
    "print(len(newBeatLib))\n",
    "print(len(total_data))\n",
    "print(len(total_data[0]))\n",
    "print(len(total_data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 19, 22, 23, 30, 40, 45, 60, 80, 120, 180, 240, 300, 360, 480, 600, 720]\n"
     ]
    }
   ],
   "source": [
    "#optimized beat list.\n",
    "beatList=[]\n",
    "\n",
    "for key, val in newBeatLib.items():\n",
    "    beatList.append(key)\n",
    "\n",
    "beatList.sort()\n",
    "print(beatList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating one-hot vectors based on the optimized data.\n",
    "numClasses=(88-noteTransPose)*len(newBeatLib)\n",
    "\n",
    "def one_hotify(element, _beatLib):\n",
    "    beat, note=element\n",
    "    a=np.zeros(numClasses)\n",
    "    a[note*(_beatLib.index(beat)+1)]=1\n",
    "    return a\n",
    "\n",
    "new_train_x=[]\n",
    "new_train_y=[]\n",
    "new_test_x=[]\n",
    "new_test_y=[]\n",
    "\n",
    "for i in train_x:\n",
    "    i_s=[]\n",
    "    for j in i:\n",
    "        i_s.append(one_hotify(j, beatList))\n",
    "    new_train_x.append(i_s)\n",
    "for i in train_y:\n",
    "    i_s=[]\n",
    "    for j in i:\n",
    "        i_s.append(one_hotify(j, beatList))\n",
    "    new_train_y.append(i_s)\n",
    "for i in test_x:\n",
    "    i_s=[]\n",
    "    for j in i:\n",
    "        i_s.append(one_hotify(j, beatList))\n",
    "    new_test_x.append(i_s)\n",
    "for i in test_y:\n",
    "    i_s=[]\n",
    "    for j in i:\n",
    "        i_s.append(one_hotify(j, beatList))\n",
    "    new_test_y.append(i_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 100, 884)\n"
     ]
    }
   ],
   "source": [
    "#final shape of the input\n",
    "print(np.asarray(new_train_x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### below is standard tensorflow stuff.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=35\n",
    "def getBatch(x,y):\n",
    "    start=(getBatch.index*batch_size)%350\n",
    "    getBatch.index+=1\n",
    "    end=start+batch_size\n",
    "    \n",
    "    seq=np.arange(start,end)\n",
    "    np.random.shuffle(seq)\n",
    "    train=seq[:30]\n",
    "    val=seq[30:]\n",
    "    \n",
    "    tx, ty, vx, vy = [], [], [], []\n",
    "    for i in train:\n",
    "        tx.append(x[i])\n",
    "        ty.append(y[i])\n",
    "    for i in val:\n",
    "        vx.append(x[i])\n",
    "        vy.append(y[i])\n",
    "    return tx, ty, vx, vy\n",
    "getBatch.index=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-b82a7492599b>:12: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    }
   ],
   "source": [
    "num_hidden=256\n",
    "num_classes=len(new_train_x[0][0])\n",
    "learning_rate=0.01\n",
    "LSTM_LAYER_SIZE=1\n",
    "FORGET_BIAS=1\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X=tf.placeholder(tf.float32, [None,None,num_classes])\n",
    "Y=tf.placeholder(tf.float32, [None,None,num_classes])\n",
    "\n",
    "cells = [rnn.BasicLSTMCell(num_hidden, forget_bias=FORGET_BIAS) for _ in range(LSTM_LAYER_SIZE)]\n",
    "cells = rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "cbatch_size=tf.shape(X)[0]\n",
    "init_state=cells.zero_state(cbatch_size, dtype=tf.float32)\n",
    "\n",
    "output, last_state = tf.nn.dynamic_rnn(inputs=X, cell=cells, dtype=tf.float32, initial_state=init_state)\n",
    "\n",
    "logits=tf.contrib.layers.fully_connected(output, num_classes, activation_fn=tf.nn.relu)\n",
    "\n",
    "scores=tf.nn.softmax(logits)\n",
    "my_predictions=tf.one_hot(tf.argmax(scores, 2), num_classes)\n",
    "\n",
    "losses=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y)\n",
    "tot_loss=tf.reduce_mean(losses)\n",
    "optimizer=tf.train.RMSPropOptimizer(learning_rate=learning_rate,decay=0.9).minimize(tot_loss)\n",
    "\n",
    "sess=tf.Session()\n",
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1140, 1024) dtype=float32_ref>\n",
      "(1140, 1024)\n",
      "2\n",
      "1140\n",
      "1024\n",
      "1167360\n",
      "<tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "(1024,)\n",
      "1\n",
      "1024\n",
      "1024\n",
      "<tf.Variable 'fully_connected/weights:0' shape=(256, 884) dtype=float32_ref>\n",
      "(256, 884)\n",
      "2\n",
      "256\n",
      "884\n",
      "226304\n",
      "<tf.Variable 'fully_connected/biases:0' shape=(884,) dtype=float32_ref>\n",
      "(884,)\n",
      "1\n",
      "884\n",
      "884\n",
      "1395572\n"
     ]
    }
   ],
   "source": [
    "#checkign all parameters\n",
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    print(variable)\n",
    "    shape = variable.get_shape()\n",
    "    print(shape)\n",
    "    print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "    print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(midicsvWorkingPath)\n",
    "\n",
    "train_guess=[]\n",
    "    \n",
    "getBatch.index=0\n",
    "\n",
    "def train(training_step, guesses, save_step=50, view_step=3,):\n",
    "    for step in range(training_step):\n",
    "        #fetchData={'fianl_state': last_state, 'prediction': pred}\n",
    "        tx, ty, vx, vy = getBatch(new_train_x, new_train_y)\n",
    "        feedData={X: tx, Y: ty}\n",
    "\n",
    "        sess.run(optimizer, feed_dict=feedData)\n",
    "        if(step%save_step==0):\n",
    "            guesses.append(my_predictions.eval(feed_dict={X: tx+ty}, session=sess))\n",
    "            #saver.save(sess, saveAbsPath, global_step=step)\n",
    "        if(step%view_step==0):\n",
    "            tr_loss=sess.run(tot_loss, feed_dict={X:tx, Y:ty})\n",
    "            val_loss=sess.run(tot_loss, feed_dict={X:vx, Y:vy})\n",
    "            print(\"step %d, training_loss: %f validation_loss: %f\"%(step, tr_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training_loss: 1.923124 validation_loss: 1.776204\n",
      "step 10, training_loss: 1.832351 validation_loss: 1.901775\n",
      "step 20, training_loss: 1.730140 validation_loss: 1.654095\n",
      "step 30, training_loss: 1.637091 validation_loss: 1.807093\n",
      "step 40, training_loss: 1.746888 validation_loss: 1.792522\n",
      "step 50, training_loss: 1.498694 validation_loss: 1.895811\n",
      "step 60, training_loss: 1.484654 validation_loss: 1.416065\n",
      "step 70, training_loss: 1.515327 validation_loss: 1.625744\n",
      "step 80, training_loss: 1.299097 validation_loss: 1.795892\n",
      "step 90, training_loss: 1.288574 validation_loss: 1.461208\n",
      "step 100, training_loss: 1.309970 validation_loss: 1.678483\n",
      "step 110, training_loss: 1.271850 validation_loss: 1.331325\n",
      "step 120, training_loss: 1.208103 validation_loss: 0.922279\n",
      "step 130, training_loss: 1.120771 validation_loss: 1.241629\n",
      "step 140, training_loss: 1.059495 validation_loss: 1.116532\n",
      "step 150, training_loss: 1.026877 validation_loss: 1.381041\n",
      "step 160, training_loss: 0.995571 validation_loss: 0.921490\n",
      "step 170, training_loss: 1.031026 validation_loss: 0.881028\n",
      "step 180, training_loss: 0.830454 validation_loss: 1.388083\n",
      "step 190, training_loss: 0.854122 validation_loss: 0.946287\n",
      "step 200, training_loss: 0.826865 validation_loss: 0.876126\n",
      "step 210, training_loss: 0.823362 validation_loss: 0.900678\n",
      "step 220, training_loss: 0.746549 validation_loss: 0.915443\n",
      "step 230, training_loss: 0.741424 validation_loss: 0.847749\n",
      "step 240, training_loss: 0.709650 validation_loss: 0.797111\n",
      "step 250, training_loss: 0.669427 validation_loss: 0.894906\n",
      "step 260, training_loss: 0.632635 validation_loss: 0.592881\n",
      "step 270, training_loss: 0.624626 validation_loss: 0.782571\n",
      "step 280, training_loss: 0.597232 validation_loss: 0.660581\n",
      "step 290, training_loss: 0.595212 validation_loss: 0.867811\n",
      "step 300, training_loss: 0.597761 validation_loss: 0.489531\n",
      "step 310, training_loss: 0.548095 validation_loss: 0.535041\n",
      "step 320, training_loss: 0.559751 validation_loss: 0.552332\n",
      "step 330, training_loss: 0.496770 validation_loss: 0.616975\n",
      "step 340, training_loss: 0.461610 validation_loss: 0.745546\n",
      "step 350, training_loss: 0.487610 validation_loss: 0.667348\n",
      "step 360, training_loss: 0.447562 validation_loss: 0.568332\n",
      "step 370, training_loss: 0.423039 validation_loss: 0.353650\n",
      "step 380, training_loss: 0.414934 validation_loss: 0.556791\n",
      "step 390, training_loss: 0.399122 validation_loss: 0.419554\n",
      "step 400, training_loss: 0.386961 validation_loss: 0.538243\n",
      "step 410, training_loss: 0.403371 validation_loss: 0.562164\n",
      "step 420, training_loss: 0.438000 validation_loss: 0.421185\n",
      "step 430, training_loss: 0.351550 validation_loss: 0.407070\n",
      "step 440, training_loss: 0.367006 validation_loss: 0.340931\n",
      "step 450, training_loss: 0.336613 validation_loss: 0.289692\n",
      "step 460, training_loss: 0.350323 validation_loss: 0.289482\n",
      "step 470, training_loss: 0.328537 validation_loss: 0.333426\n",
      "step 480, training_loss: 0.322195 validation_loss: 0.335436\n",
      "step 490, training_loss: 0.358600 validation_loss: 0.354184\n",
      "step 500, training_loss: 0.297719 validation_loss: 0.226097\n",
      "step 510, training_loss: 0.280466 validation_loss: 0.311212\n",
      "step 520, training_loss: 0.301730 validation_loss: 0.345836\n",
      "step 530, training_loss: 0.266329 validation_loss: 0.236188\n",
      "step 540, training_loss: 0.303933 validation_loss: 0.218153\n",
      "step 550, training_loss: 0.284643 validation_loss: 0.300023\n",
      "step 560, training_loss: 0.243678 validation_loss: 0.301812\n",
      "step 570, training_loss: 0.250547 validation_loss: 0.334755\n",
      "step 580, training_loss: 0.305325 validation_loss: 0.147390\n",
      "step 590, training_loss: 0.250883 validation_loss: 0.284562\n",
      "step 600, training_loss: 0.222247 validation_loss: 0.448377\n",
      "step 610, training_loss: 0.236893 validation_loss: 0.219994\n",
      "step 620, training_loss: 0.262902 validation_loss: 0.153348\n",
      "step 630, training_loss: 0.256212 validation_loss: 0.157601\n",
      "step 640, training_loss: 0.216629 validation_loss: 0.332757\n",
      "step 650, training_loss: 0.220784 validation_loss: 0.166039\n",
      "step 660, training_loss: 0.260744 validation_loss: 0.394582\n",
      "step 670, training_loss: 0.255863 validation_loss: 0.238481\n",
      "step 680, training_loss: 0.258629 validation_loss: 0.206903\n",
      "step 690, training_loss: 0.233693 validation_loss: 0.178332\n",
      "step 700, training_loss: 0.271519 validation_loss: 0.275898\n",
      "step 710, training_loss: 0.218393 validation_loss: 0.197983\n",
      "step 720, training_loss: 0.223455 validation_loss: 0.175059\n",
      "step 730, training_loss: 0.244696 validation_loss: 0.268180\n",
      "step 740, training_loss: 0.219901 validation_loss: 0.333688\n",
      "step 750, training_loss: 0.177829 validation_loss: 0.337802\n",
      "step 760, training_loss: 0.203403 validation_loss: 0.251766\n",
      "step 770, training_loss: 0.217693 validation_loss: 0.282477\n",
      "step 780, training_loss: 0.260190 validation_loss: 0.192441\n",
      "step 790, training_loss: 0.203426 validation_loss: 0.165407\n",
      "step 800, training_loss: 0.193934 validation_loss: 0.137145\n",
      "step 810, training_loss: 0.214273 validation_loss: 0.090834\n",
      "step 820, training_loss: 0.249781 validation_loss: 0.272966\n",
      "step 830, training_loss: 0.226702 validation_loss: 0.195981\n",
      "step 840, training_loss: 0.199745 validation_loss: 0.097453\n",
      "step 850, training_loss: 0.162177 validation_loss: 0.343238\n",
      "step 860, training_loss: 0.232898 validation_loss: 0.192492\n",
      "step 870, training_loss: 0.276192 validation_loss: 0.149550\n",
      "step 880, training_loss: 0.190693 validation_loss: 0.097865\n",
      "step 890, training_loss: 0.162494 validation_loss: 0.255050\n",
      "step 900, training_loss: 0.203857 validation_loss: 0.086512\n",
      "step 910, training_loss: 0.233426 validation_loss: 0.346512\n",
      "step 920, training_loss: 0.177647 validation_loss: 0.334940\n",
      "step 930, training_loss: 0.191229 validation_loss: 0.133040\n",
      "step 940, training_loss: 0.174907 validation_loss: 0.216033\n",
      "step 950, training_loss: 0.208198 validation_loss: 0.141747\n",
      "step 960, training_loss: 0.251725 validation_loss: 0.141981\n",
      "step 970, training_loss: 0.175642 validation_loss: 0.322799\n",
      "step 980, training_loss: 0.189513 validation_loss: 0.150040\n",
      "step 990, training_loss: 0.152283 validation_loss: 0.310230\n"
     ]
    }
   ],
   "source": [
    "train(1000, train_guess, save_step=100, view_step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create(seed, length=100):\n",
    "    guesses=[]\n",
    "    fetchData= {'final_state': last_state,'prediction': my_predictions}\n",
    "    element=[60,65-noteTransPose+seed]\n",
    "    d=one_hotify(element,beatList)\n",
    "    init_input=np.reshape(d,(1,1,884))\n",
    "    feedData={X: init_input}\n",
    "    \n",
    "    eval_out=sess.run(fetchData, feedData)\n",
    "    guesses=[]\n",
    "    guesses.append(eval_out['prediction'])\n",
    "    \n",
    "    \n",
    "    next_state=eval_out['final_state']\n",
    "    for step in range(1,length):\n",
    "        feedData={X: guesses[-1], init_state:next_state}\n",
    "        eval_out=sess.run(fetchData,feedData)\n",
    "        guesses.append(eval_out['prediction'])\n",
    "        \n",
    "        next_state=eval_out['final_state']\n",
    "    return guesses\n",
    "create_guess=[]\n",
    "for i in range(-10,10):\n",
    "    create_guess.append(create(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEATLIB=sorted(newBeatLib.items(), key=lambda kv: kv[1])\n",
    "list.reverse(BEATLIB)\n",
    "\n",
    "def reverse_one_hotify(element, _beatLib, orderLIB=BEATLIB):\n",
    "    index=np.argmax(element)\n",
    "    for beat, _ in orderLIB:\n",
    "        if(index%(_beatLib.index(beat)+1)==0):\n",
    "            tempNote=index//(_beatLib.index(beat)+1)\n",
    "            if(0<=tempNote and tempNote<88-noteTransPose):\n",
    "                return [beat, tempNote+noteTransPose]\n",
    "    raise ValueError('note not identified')\n",
    "\n",
    "CREATED=[]\n",
    "for track in create_guess:\n",
    "    TRACK=[]\n",
    "    for element in track:\n",
    "        TRACK.append(reverse_one_hotify(element, beatList))\n",
    "    CREATED.append(TRACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60, 61], [60, 63], [60, 64], [60, 66], [60, 68], [60, 70], [30, 43], [60, 86], [240, 74], [120, 74], [60, 71], [60, 66], [60, 64], [60, 62], [60, 64], [60, 67], [60, 65], [60, 64], [60, 62], [60, 71], [60, 81], [60, 65], [60, 67], [240, 69], [60, 67], [60, 72], [60, 67], [60, 65], [60, 67], [60, 75], [60, 74], [60, 72], [60, 71], [60, 74], [60, 72], [60, 74], [60, 70], [60, 69], [60, 67], [60, 65], [60, 71], [60, 70], [60, 69], [60, 67], [60, 65], [60, 71], [60, 72], [60, 74], [60, 76], [60, 72], [60, 74], [60, 77], [60, 70], [60, 72], [60, 74], [60, 76], [60, 77], [60, 76], [60, 74], [60, 72], [60, 70], [60, 68], [60, 67], [60, 68], [60, 65], [60, 64], [60, 65], [60, 72], [60, 68], [60, 67], [60, 68], [60, 77], [60, 75], [60, 73], [60, 72], [60, 70], [60, 68], [60, 67], [60, 65], [60, 67], [60, 63], [60, 62], [60, 63], [60, 70], [60, 67], [60, 65], [60, 67], [60, 75], [60, 73], [60, 72], [60, 70], [60, 68], [60, 67], [60, 65], [60, 64], [120, 65], [120, 69], [120, 70], [60, 81], [120, 73]]\n"
     ]
    }
   ],
   "source": [
    "print(CREATED[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "my_track=[]\n",
    "\n",
    "def createOutputData1(track): #support only 1 dimension\n",
    "    output=[]\n",
    "    \n",
    "    pBeat=0\n",
    "    \n",
    "    for beat, note in track:\n",
    "        output.append([pBeat,[note]])\n",
    "        pBeat+=beat\n",
    "    return output\n",
    "\n",
    "for track in CREATED:\n",
    "    my_track.append(createOutputData1(track))\n",
    "\n",
    "print(len(my_track[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "outRelPath=r'\\midicsv-1.1\\out'\n",
    "outAbsPath=basePath+outRelPath\n",
    "\n",
    "def writeTrack(f, hardtrackNum, softTrackNum, source):##int values are sent as str\n",
    "    fresh=True\n",
    "    last_pitch=''\n",
    "    last_timeStamp=''\n",
    "    f.write(hardtrackNum+', 0, Start_track\\n')\n",
    "    f.write(hardtrackNum+', 0, Title_t, \"Track '+str(int(hardtrackNum)-1)+'\"\\n')\n",
    "    f.write(hardtrackNum+', 0, Program_c, '+str(int(hardtrackNum)-2)+', 6\\n') # 6 specifies instrument type?\n",
    "    for [key, val] in source:\n",
    "        if(len(val)>int(softTrackNum)):##only if val has current track key to play\n",
    "            if(fresh==False):\n",
    "                f.write(hardtrackNum+', '+str(int(key)-1)+', Note_off_c, '+str(int(hardtrackNum)-2)+', '+last_pitch+', 0\\n')\n",
    "            else:\n",
    "                fresh=False\n",
    "            last_pitch=str(val[int(softTrackNum)])\n",
    "            f.write(hardtrackNum+', '+str(key)+', Note_on_c, '+str(int(hardtrackNum)-2)+', '+str(val[int(softTrackNum)])+', 88\\n')\n",
    "            last_timeStamp=str(key)\n",
    "    f.write(hardtrackNum+', '+str(int(last_timeStamp)+40)+', Note_off_c, '+str(int(hardtrackNum)-2)+', '+last_pitch+', 0\\n')\n",
    "    f.write(hardtrackNum+', '+str(int(last_timeStamp)+40)+', End_track\\n')\n",
    "\n",
    "def writeCSV(track,fileName):\n",
    "    PathHeader=r'\\\\'\n",
    "    filePath=outAbsPath+PathHeader+fileName\n",
    "    f=open(filePath,\"w\")\n",
    "    maxTrackNum=1\n",
    "    f.write('0, 0, Header, 1, '+'2 '+', '+'240'+'\\n')\n",
    "    f.write('1, 0, Start_track\\n')\n",
    "    f.write('1, 0, Time_signature, 4, 2, 24, 8\\n')\n",
    "    f.write('1, 0, Tempo, '+'714285'+'\\n')#random number\n",
    "    f.write('1, 0, SMPTE_offset, 64, 0, 0, 0, 100\\n')\n",
    "    f.write('1, 0, End_track\\n')\n",
    "    \n",
    "    curHardTrackNum=2\n",
    "    ##write right hand\n",
    "    for i in range(0,1):\n",
    "        writeTrack(f,str(curHardTrackNum),str(i),track)\n",
    "        curHardTrackNum+=1\n",
    "\n",
    "    f.write('0, 0, End_of_file')\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(my_track)):\n",
    "    writeCSV(my_track[i], \"created_\"+str(i)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(midicsvWorkingPath)\n",
    "for i in range(len(my_track)):\n",
    "    fileName=\"created_\"+str(i)+\".csv\"\n",
    "    newFileName=\"created_\"+str(i)\n",
    "    cmd='csvmidi ./out/'+fileName+' ./out/'+newFileName+'.mid'\n",
    "    os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
